import copy
import numpy as np
from numpy import linalg as la
from sklearn.covariance import EmpiricalCovariance as Ecov
import time
from matplotlib import pyplot as plt

def conj_grad(A,b):
    """perform conjugate gradient descent on Ax=b until 
    error tolerance is reached"""
    [m,n] = np.shape(A)
    cov = la.inv(A)
    #initialize state vectors
    x = np.zeros([m,1])
    y = np.zeros([m,1])
    c = np.zeros([m,1])
    #initialize residual
    r = b - np.dot(A,x)
    #initialize transpose of residual
    rsold = np.dot(np.transpose(r),r)
    #initialize search direction
    p = r
    #calculate A*p vector
    A_p = np.dot(A,p)
    #normalizing constant
    d = np.dot(np.transpose(p),np.dot(A,p))
    #store approximation generated by formula
    CG_cov = np.zeros([m,n])
    CG_cov_min = np.zeros([m,n])
    rel_err_min = 1
    #iterate m times (or until convergence)
    for i in range(8):
        #print("iteration {}/{}".format(i,m))
        gamma = (rsold)/d
        x = x + np.dot(p,gamma)
        z = np.random.randn()
        y = y + (z/np.sqrt(d)) * p
        #c_k = A * y_k
        c = c + (z/np.sqrt(d)) * (np.matmul(A,p))
        #store old r to calculate new beta
        r = r - gamma*A_p
        rsnew = np.dot(np.transpose(r),r)
        r_rel = la.norm(r)/la.norm(b)
        #is the magnitude of the residual < 1e-16?
        if la.norm(r,2) < 1e-4:
            print("converged at iteration {} with relative residual {}".format(i,r_rel))
            break
        #calculate new beta
        beta = rsnew/rsold
        #update rsold constant
        rsold = rsnew
        #approximate covariance matrix iteratively
        cov_approx_part = (1/d) * np.matmul(p,np.transpose(p))
        CG_cov += cov_approx_part
        rel_err = la.norm(cov - CG_cov)/la.norm(cov)
        if (rel_err < rel_err_min):
            CG_cov_min = CG_cov
            rel_err_min = rel_err
        elif (rel_err > 1.3*rel_err_min):
            CG_cov = CG_cov_min
            print("conjugacy lost, reverting")
        #print("relative error at iteration {} is {}".format(i,rel_err))
        #calculate new search direction
        p = r + beta*p
        #calculate A*p vector
        A_p = np.dot(A,p)
        #calculate new normalization constant
        d = np.dot(np.transpose(p),A_p)
    return x,y,c,CG_cov

def cg_sample(A,sample_num):
    #sample from A sample_num times using a different b each time
    #return sample num number of c_k ~ N(0,A). Take empirical covariance of this. 
    CG_samples = []
    c_samples = []
    [dims,dims] = np.shape(A)
    for i in range(sample_num):
        print("sample {}/{}".format(i,sample_num))
        b = np.random.randn(dims,1)
        [x,y,c_k,CG_cov] = conj_grad(A,b)
        #CG_samples.append(CG_cov)
        c_samples.append(c_k)
    #want to take the mean over the first axis (average the matrices)...
    #CG_cov = np.mean(CG_samples,0)
    print("shape of c samples is {}".format(np.shape(c_samples)))
    [x,y,z] = np.shape(c_samples)
    c_samples = np.reshape(c_samples,[x,y])
    print("shape of flattened c samples is {}".format(np.shape(c_samples)))
    CG_cov_emp  = Ecov().fit(c_samples).covariance_
    return CG_cov_emp

def cg_plot(A,b):
    
    #########################################
    # solver solution plot                  #
    #########################################
    cov = A 
    x_a = np.matmul(la.inv(cov),b)
    x_cg,y,c_k,CG_cov = conj_grad(A,b)
    x_a = np.sort(np.ravel(x_a))
    x_cg = np.sort(np.ravel(x_cg))
    rel_err = la.norm(x_a - x_cg)/la.norm(x_a)
    plt.plot(x_a,label="analytic solution")
    plt.plot(x_cg,marker='1',linestyle='none',label='CG solution')
    plt.legend()
    plt.title("Analytic solution and cg solution for Ax=b, rel err {}".format(rel_err))
    plt.show()
    
    #########################################
    #    COVARIANCE EIGENVALUE PLOT         #
    #########################################
    CG_cov_emp = cg_sample(A,int(1e5))
    rel_err = la.norm(cov - CG_cov)/la.norm(cov)
    eigs, eigvecs = la.eigh(cov)
    #just want to plot the last 20 eigs
    eigs = np.sort(eigs)[-20:]
    plt.semilogy(eigs,'o',mfc='none',label="actual eigenvalues")
    #now from formula covariance 
    #eigs_CG, eigvecs_CG = la.eigh(CG_cov)
    #eigs_CG = np.ma.masked_where(eigs_CG<1e-5,eigs_CG)
    #plt.semilogy(np.sort(eigs_CG),marker='1',linestyle='none',label='CG analytic eigenvalues')
    #now from empirical covariance 
    eigs_CG_emp, eigvecs_CG_emp = la.eigh(CG_cov_emp)
    #eigs_CG_emp = np.ma.masked_where(eigs_CG_emp < 1e-5,eigs_CG_emp)
    #just want to plot the last 20 eigs
    eigs_cg_emp = np.sort(eigs_CG_emp)[-20:]
    plt.semilogy(eigs_cg_emp,marker='2',linestyle='none',label='CG empirical eigenvalues')
    plt.legend()
    plt.title("Eigenvalues after 8 iterations".format(rel_err))
    plt.show()

if __name__=="__main__":
    #generate squared exponential covariance matrix
    x = 100
    s = np.linspace(-3,3,x)
    epsilon = 1e-6
    gauss_matrix = np.zeros([x,x])

    for i in range(x):
        for j in range(x):
            arg = -((s[i] - s[j])**2)/(2*(1.5)**2)
            gauss_matrix[i,j] = 2*np.exp(arg)

    gauss_matrix += epsilon*np.eye(x)
    A = gauss_matrix
    [dims,dims] = np.shape(A)
    vals = [-1, 1]
    #randomly draw from vals into a row vector
    b = np.random.choice(vals,[dims,1])
    cg_plot(A,b)
